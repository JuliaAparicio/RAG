# ğŸ§  RAG Pipeline com LangChain + LangGraph

ImplementaÃ§Ã£o de um sistema de Retrieval-Augmented Generation (RAG) utilizando LangChain, LangGraph e OpenAI, integrando mÃºltiplas fontes de conhecimento (PDF e conteÃºdo externo) para geraÃ§Ã£o de respostas contextualizadas e fundamentadas.

---

## ğŸš€ VisÃ£o Geral

Este projeto implementa um pipeline completo de RAG capaz de:

- Ingerir documentos de diferentes fontes
- Dividir o conteÃºdo em chunks estratÃ©gicos
- Gerar embeddings semÃ¢nticos
- Armazenar em banco vetorial
- Recuperar trechos relevantes via similaridade
- Injetar contexto no prompt
- Gerar respostas fundamentadas usando LLM

A orquestraÃ§Ã£o do fluxo foi realizada com **LangGraph**, organizando o sistema em etapas modulares de recuperaÃ§Ã£o e geraÃ§Ã£o.

---

## ğŸ—ï¸ Arquitetura do Sistema

O pipeline segue as seguintes etapas:

### 1ï¸âƒ£ IngestÃ£o de Dados

Fontes utilizadas:

- ğŸ“„ PDF (TCC sobre MatemÃ¡tica / Ãlgebra Booleana)
- ğŸ“¦ ConteÃºdo externo carregado via loader

Ferramentas:
- `PyPDFLoader`
- `YoutubeLoaderDL`
- `langchain_community.document_loaders`

---

### 2ï¸âƒ£ Chunking

UtilizaÃ§Ã£o de:

- `RecursiveCharacterTextSplitter`
- `chunk_size = 1000`
- `chunk_overlap = 200`

Objetivo:
- Preservar contexto semÃ¢ntico
- Reduzir perda de informaÃ§Ã£o
- Melhorar precisÃ£o da busca vetorial

---

### 3ï¸âƒ£ VetorizaÃ§Ã£o

- Modelo de embeddings: `text-embedding-ada-002`
- Banco vetorial: `InMemoryVectorStore`

Cada chunk Ã© transformado em embedding e armazenado para posterior busca por similaridade semÃ¢ntica.

---

### 4ï¸âƒ£ RecuperaÃ§Ã£o (Retrieval)

```python
def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}
